---
title: <center style="font-size:30px;font-style:normal;color:black;">Accompanying Code 04:</center>
subtitle: <center style="font-size:30px;font-style:normal;color:#0E0E7D;">Multi-observer Reliability of Visual Classification</center>
 &nbsp;
author: |
  <center style="font-style:normal;">
  <a style="font-size:22px;color:#337ab7;text-decoration: underline;"href="http://smith-vidaurre.com/">Grace Smith-Vidaurre</a><sup><span style="font-size:12px;color:black;text-decoration:none!important;">1-4*</span></sup>
  &nbsp;
  <center style="font-size:18px;font-style:normal;color:black;"><sup><span style="font-size:12px;color:black;">1</span></sup>Department of Biology, New Mexico State University</center>
  <center style="font-size:18px;font-style:normal;color:black;"><sup><span style="font-size:12px;color:black;">2</span></sup>Laboratory of Neurogenetics of Language, Rockefeller University</center>
  <center style="font-size:18px;font-style:normal;color:black;"><sup><span style="font-size:12px;color:black;">3</span></sup>Field Research Center, Rockefeller University</center>
  <center style="font-size:18px;font-style:normal;color:black;"><sup><span style="font-size:12px;color:black;">4</span></sup>Department of Biological Sciences, University of Cincinnati</center>
  <br />
  <center style="font-size:18px;"><sup style="font-size:12px;">*</sup>gsvidaurre@gmail.com</center>
  &nbsp;
date: <center style="font-size:22px;font-style:normal";>`r format(Sys.time(), '%d %B, %Y')`</center>
  <br />
output: 
  html_document:
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
---

<style type="text/css">

a:hover {
  color: #23527c !important;
}

h1.title {
  font-size: 32px;
  color: black;
  font-weight: normal;
}

h1 {
   color: black;
   font-size: 26px;
   font-weight: normal;
}

h2 {
   color: black;
   font-size: 24px;
   font-weight: bold;
}

h3 {
   color: black;
   font-size: 20px;
   font-weight: normal;
}

h4 {
   color: black;
   font-size: 20px;
   font-weight: normal;
}

body{ /* Normal */
      font-size: 18px;
  }
  
code.r{ /* Code block */
    font-size: 18px;
}
</style>

```{r global options, include = FALSE}

knitr::opts_knit$set(root.dir = "/home/gsvidaurre/Desktop/GitHub_repos/identity-information-post-introduction", echo = TRUE, include = TRUE, eval = TRUE)

knitr::opts_chunk$set(root.dir = "/home/gsvidaurre/Desktop/GitHub_repos/identity-information-post-introduction", echo = TRUE, include = TRUE, eval = TRUE)

```

Here we validated visual classification of potential repeated sampling of individuals. We assessed multi-observer reliability and the number of potential repeated individuals identified per range. For the latter, we decided to use visual classification results over clusters identified by the Gaussian mixture models, which upon preliminary visual inspection (see IdentityEncoding_AccompanyingCode_01_SummaryStatistics_PreliminaryVisualInspection.Rmd), tended to group together calls that likely represented different individuals, and split calls into different clusters that likely represented the same individual. As such, we relied on visual classification (which was informed by the clusters generated through Gaussian mixture models) as a finer-scale filter of potential repeated individuals. See the methods and appendix of the associated publication for more information about these analyses.

Check out Github repositories from previous work for related analyses and code:

- [gsvidaurre/strong-individual-signatures](https://github.com/gsvidaurre/strong-individual-signatures)<a href='#References'><sup>[1]</sup></a>

- [gsvidaurre/simpler-signatures-post-invasion](https://github.com/gsvidaurre/simpler-signatures-post-invasion)<a href='#References'><sup>[2]</sup></a>

Please cite the associated papers and code (see DOIs on GitHub) if the code or analyses in these 3 repositories are useful for your own research.

Clean environment, load packages, initialize fundamental objects.
```{r echo = TRUE, eval = TRUE, message = FALSE}

rm(list = ls())

X <- c("ggplot2", "pbapply",  "dplyr", "data.table", "tidyverse", "rlist", "rdrop2", "plotrix", "orddom", "Rmisc", "grid", "gridExtra", "egg", "ggplotify", "pracma", "irr", "knitr")

invisible(lapply(X, library, character.only = TRUE))

# Suppress summarise warnings from dplyr
options(dplyr.summarise.inform = FALSE)

path <- "/media/gsvidaurre/MYIOPSITTA/R/VocalLearning_PostDisruption/Data"
seed <- 401
cores <- parallel::detectCores() - 2
# cores

# Dropbox authorization token and path
app_path <- "/home/gsvidaurre/Desktop/GitHub_repos/vocal-learning-invasion/pot-rep-indiv-filter"
token <- readRDS(file.path(app_path, "token.rds"))
drop_path <- "Smith-Vidaurre_Grace/CulturalEvolution_PostInvasion/Detect_RepeatedIndividualSampling"
gpath <- "/media/gsvidaurre/MYIOPSITTA/R/VocalLearning_PostDisruption/Graphics"

```

Read in the extended selection table (EST) that contains metadata and wave objects for pre-processed native and introduced range calls across the individual and site scales.
```{r echo = TRUE, eval = TRUE}

nat_int_est <- readRDS(file.path(path, "monk_parakeet_contactCalls_rangeComparison_extSelTable.RDS"))
glimpse(nat_int_est)

```

# Filtering out some calls

I dropped individual scale calls that had been added to the site scale (1 call per known repeatedly sampled individual in the final individual scale dataset, suffix "site_scale").
```{r chunk3}

# 31 calls with the suffix "_site_scale". Not all of these belonged to the individuals in the final individual scale dataset, since some individuals that we repeatedly sampled were dropped due to low sample sizes after pre-processing
nat_int_est %>%
  dplyr::filter(social_scale == "Site") %>% 
  dplyr::filter(grepl("_site_scale", sound.files)) %>%
  pull(sound.files) %>%
  length()

# Get the "_site_scale" calls for the final dataset of known repeatedly sampled individuals
indiv_ids <- nat_int_est %>%
  dplyr::filter(social_scale == "Individual") %>%
  pull(Bird_ID) %>%
  unique()

# 17 total: 8 native and 9 introduced range
indiv_ids

# INT-UM1, INT-UM6, and INT-UM19 were each the sole bird sampled at sites BART, ASCA, and CAME, respectively. These sites were not included at the site scale, so these individuals did not have any "_site_scale" calls 
# This leaves 14 calls to drop, each representing a call for a different repeatedly sampled individual included in the site scale dataset
drop_is_calls <- nat_int_est %>%
  dplyr::filter(social_scale == "Site") %>%
  dplyr::filter(grepl("_site_scale", sound.files)) %>%
  dplyr::filter(Bird_ID %in% indiv_ids) %>%
  pull(sound.files)

length(drop_is_calls)

# Checking, looks good
nat_int_est %>%
  as_tibble() %>% 
  dplyr::filter(sound.files %in% drop_is_calls) %>%
  dplyr::select(sound.files, Bird_ID) %>%
  kable(align = rep("c", nrow(.)))

# Drop these calls from the extended selection table and continue with analyses
# nat_int_est <- nat_int_est %>%
  # dplyr::filter(!sound.files %in% drop_is_calls)

# Don't use tidyverse since I want to filter both the data frame and the .wav objects in the EST
nat_int_est <- nat_int_est[-grep(paste(paste("^", drop_is_calls, "$", sep = ""), collapse = "|"), nat_int_est$sound.files), ]

# 1582 calls remain out of the original 1596 calls read in above
dim(nat_int_est)
class(nat_int_est)

```

### AM4.1

![Workflow for multi-observer visual classification of potential repeated individuals at the site scale](/home/gsvidaurre/Desktop/GitHub_repos/identity-information-post-introduction/images/AdditionalMaterials_MultiobserverVisualInspection_Workflow.tif)

Workflow of the approach we used to address potential repeated sampling of unmarked individuals at the site scale. Native and introduced range calls were taken through this same workflow together, including previously published calls from the native range (2017) and the introduced range (2004) that were visually inspected for potential repeated individual sampling by a single observer (a different observer per range) in previous work.

Download co-author's data from Dropbox.
```{r echo = TRUE, eval = FALSE}

drop_files <- drop_dir(drop_path) %>%
  dplyr::select(name) %>%
  pull(name)

drop_files

# Iterate over the files and download them from Dropbox one by one
invisible(pblapply(1:length(drop_files), function(x){
  drop_download(path = file.path(drop_path, drop_files[x]), dtoken = token, local_path = path, overwrite = TRUE)
}))

```

Read in the .json files and concatenate results across collaborators into a single data frame.
```{r echo = TRUE, eval = TRUE}

# Retain each .fson file for co-author. For VP, used the most recent file vp-2020-05-18_18-50-40.json
file_nms <- list.files(path, pattern = ".json$")
file_nms

users <- sapply(1:length(file_nms), function(i){
  strsplit(file_nms[i], split = "-")[[1]][1]
})
users

```

# Training accuracy

Training accuracy summary statistics across users. Here, pulled training classification accuracy from the last training round before testing only. Note that ASM's training data was only partially saved (see below).
```{r echo = TRUE, eval = FALSE}

# Extract calls across classes and compare to the number of input calls
# Initialize testing data  
train_imgs <- read.csv(file.path(app_path, "training_images.csv"))
# glimpse(train_imgs)

train_accur <- rbindlist(lapply(1:length(users), function(x){
  
  res <- list.load(file.path(path, file_nms[x]))
  # str(res)
  
  # Get user accuracy from the very last round of training
  train_nms <- names(res)[-grep("Testing", names(res))]
  train_nm <- train_nms[length(train_nms)]
  # train_nm

  # accur_list <- rbindlist(lapply(1:length(train_nm), function(i){
    
  tmp <- res[[train_nm]][grep("accuracy", names(res[[train_nm]]))]
  # names(tmp)
  # str(tmp)
    
  # Get the user (not baseline) accuracies
  wh <- which(sapply(tmp[[1]], function(X){
    grepl("user", X$accur_type)
  }))
      
  # These are user classification accuracies per known repeatedly sampled individual per training site
  tmp <- tmp[[1]][wh]
  # str(tmp)
    
  # Iterate over birds to extract classification accuracy (percent right)
  tmp2 <- rbindlist(lapply(1:length(tmp), function(z){
      
    tmp_df <- data.frame(observer = users[x]) %>%
      dplyr::mutate(
        training_iteration = train_nm,
        site_year = tmp[[z]][[grep("train_page", names(tmp[[z]]))]],
        indiv = tmp[[z]][[grep("indiv", names(tmp[[z]]))]], 
        percent_right = tmp[[z]][[grep("percent_right", names(tmp[[z]]))]]
      )
      
    return(tmp_df)
      
  }))
    
  return(tmp2)

}))

glimpse(train_accur)

```

Get training accuracy summary statistics.

Mean accuracy across individuals and observers. Very similar even when ASM's partial training data is dropped.
```{r echo = TRUE, eval = TRUE}

# With ASM training data
train_accur %>%
   dplyr::summarise(
    overall_mean_accur = round(mean(percent_right), 2),
    se_accur = round(std_err(percent_right), 2)
  )

# Without ASM training data: results are very similar
train_accur %>%
  filter(observer != "asm") %>%
  droplevels() %>%
  dplyr::summarise(
    overall_mean_accur = round(mean(percent_right), 2),
    se_accur = round(std_err(percent_right), 2)
  )

```

Mean and SE accuracy across individuals by observer.
```{r echo = TRUE, eval = TRUE}

train_accur %>%
  group_by(observer) %>%
  dplyr::summarise(
    mean_accur = round(mean(percent_right), 2),
    se_accur = round(std_err(percent_right), 2)
  )
  
```

# Validation analyses

## Multi-observer reliability

For the analyses below, only considered classes as potential repeated individuals if assigned more than one call.

### AM4.2

![Wokflow for validation analyses of multi-observer reliability](/home/gsvidaurre/Desktop/GitHub_repos/identity-information-post-introduction/images/AdditionalMaterials_MultiobserverValidation_Workflow.tif)

Workflow used to validate our multi-observer visual inspection approach to identification of potential repeated sampling in individuals per range.

Use the 4 sites evaluated across users to calculate multiobserver reliability. These 4 sites were: CISN_2017 (native, smaller sample size), BALL_2004 (introduced, smaller sample size), SOCC_2019 (introduced, large sample size), SOCC_2004 (introduced, large sample size). I thought I had picked 2 introduced and 2 native, but had forgotten about that balancing when trying to pick by sample sizes. 
```{r echo = TRUE, eval = TRUE}

mo_sites <- c("CISN_2017", "BALL_2004", "SOCC_2004", "SOCC_2019")

multi_obsv <- rbindlist(lapply(1:length(users), function(x){
  
  res <- list.load(file.path(path, file_nms[x]))

  # Get the predictions for sites assessed across observers
  pred_dats <- res[["Testing"]][grep(paste(paste("^", mo_sites, "$", sep = ""), collapse = "|"), names(res[["Testing"]]))]
  # str(pred_dats)
  
  # Return a data frame per observer, with one row per call
  
  # Per site-year, get the calls in each class
  pred_df <- rbindlist(lapply(1:length(pred_dats), function(i){
    
    # Get classes, class_0 was reserved for calls considered as not belonging to any potential individuals 
    classes <- names(pred_dats[[i]][[1]])[grep("Potential|class_0", names(pred_dats[[i]][[1]]))]
    # classes  
    
    # Get the calls assigned to each class (potential repeated individual)
    class_df <- rbindlist(lapply(1:length(classes), function(z){
      
      calls <- strsplit(as.character(pred_dats[[i]][[1]][grep(classes[z], names(pred_dats[[i]][[1]]))]), split = ";")[[1]]
      
      return(data.frame(class = classes[z], call = calls))
      
    }))
    # glimpse(class_df)
    
    return(
      class_df %>%
        dplyr::mutate(
          observer = users[x],
          site_year = names(pred_dats)[i]
        ) %>%
        dplyr::select(
          observer, site_year, call, class
        )
      )
    
  }))  
  
  return(pred_df)
  
}))

glimpse(multi_obsv)

```

Multi-observer summary statistics. How many potential repeated individuals (classes) identified per site-year across observers? Only consider individuals assigned more than one call.
```{r echo = TRUE, eval = TRUE}

unique(multi_obsv$class)

mo_classes <- multi_obsv %>%
  dplyr::mutate(
    class = factor(class)
  ) %>%
  # Remove class_0, which was reserved for calls from "unique" individuals
  filter(class != "class_0") %>%
  # Only retain potental repeated individuals that were assigned more than one call
  group_by(site_year, observer, class) %>%
  dplyr::summarise(n_calls = length(call)) %>%
  ungroup() %>%
  filter(n_calls > 1) %>%
  group_by(site_year, observer) %>%
  dplyr::summarise(num_pot_rep_indiv = n_distinct(class)) %>%
  ungroup()

# Widen the data for ease of comparison
mo_classes %>%
  pivot_wider(
    names_from = "observer",
    values_from = "num_pot_rep_indiv"
  )

```

There was a lot of variation across observers in the degree of splitting versus lumping per site-year. 

Calculated Fleiss' Kappa for multiple observers. Values closer to 1 indicate higher consistency. Here, kappa was -0.0353, not only small but also negative, indicating very low consistency across observers per site-year.
```{r echo = TRUE, eval = TRUE}

# All 4 multi-observer site-years
mo_classes %>%
  # Change to n*m matrix (n subjects in rows, m raters in columns)
  pivot_wider(
    names_from = "observer",
    values_from = "num_pot_rep_indiv"
  ) %>%
  dplyr::select(-c(site_year)) %>%
  as.matrix() %>%
  kappam.fleiss(exact = FALSE, detail = FALSE)

```

Lumping and splitting across observers. 
```{r echo = TRUE, eval = TRUE}

# Colors by range
fills <- scales::alpha(c("navy", "orange"), 0.85)

# Get the mean number of potential repeated individuals per site
# Will be used to make one line per facet in plot below
dum_df <- multi_obsv %>%
  dplyr::mutate(
    site_year = as.character(site_year),
    range = ifelse(grepl("CISN", site_year), "Native", "Introduced"), 
    site_year = factor(site_year, levels = c("CISN_2017", "BALL_2004", "SOCC_2004", "SOCC_2019")),
    range = factor(range, levels = c("Native", "Introduced"))
  ) %>%
  # Remove class_0, which was reserved for calls from "unique" individuals
  filter(class != "class_0") %>%
  # Only retain potental repeated individuals that were assigned more than one call
  group_by(site_year, observer, class) %>%
  dplyr::summarise(n_calls = length(call)) %>%
  ungroup() %>%
  filter(n_calls > 1) %>%
  group_by(site_year, observer) %>%
  dplyr::summarise(num_pot_rep_indiv = n_distinct(class)) %>%
  ungroup() %>%
  group_by(site_year) %>%
  dplyr::summarise(mean_indivs = mean(num_pot_rep_indiv)) %>%
  ungroup()
dum_df

multi_obsv %>%
  dplyr::mutate(
    site_year = as.character(site_year),
    range = ifelse(grepl("CISN", site_year), "Native", "Introduced"), 
    site_year = factor(site_year, levels = c("CISN_2017", "BALL_2004", "SOCC_2004", "SOCC_2019")),
    range = factor(range, levels = c("Native", "Introduced"))
  ) %>%
  # Remove class_0, which was reserved for calls from "unique" individuals
  filter(class != "class_0") %>%
  # Only retain potental repeated individuals that were assigned more than one call
  group_by(range, site_year, observer, class) %>%
  dplyr::summarise(n_calls = length(call)) %>%
  ungroup() %>%
  filter(n_calls > 1) %>%
  group_by(range, site_year, observer) %>%
  dplyr::summarise(num_pot_rep_indiv = n_distinct(class)) %>%
  ungroup() %>%
  ggplot(aes(x = observer, y = num_pot_rep_indiv)) +
  geom_col(aes(fill = range)) +
  facet_grid(~ site_year) +
  geom_hline(data = dum_df, aes(yintercept = mean_indivs), linetype = "dotted", color = "black", size = 0.5) +
  scale_fill_manual(values = fills) +
  guides(fill = guide_legend(title = "Range")) +
  xlab("Observer") +
  ylab("Number of Potential \n Repeated Individuals") +
  # guides(fill = guide_legend(override.aes = list(size = 0.1))) +
  theme_bw() +
  theme(
    strip.text = element_text(size = 10),
    axis.title = element_text(size = 10),
    axis.text = element_text(size = 9),
    panel.grid.major = element_line(size = 0.15),
    panel.grid.minor = element_line(size = 0.15),
    axis.ticks = element_line(size = 0.15),
    legend.position = "top",
    legend.key.height = unit(0.35, "line"),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 10),
    legend.margin = margin(2, 2, 2, 2),
    legend.box.margin = margin(-5, -5, -5, -5)
  )

ggsave(file.path(gpath, "SupplementaryFigure1_MultiobserverReliability.tiff"), width = 7.48, height = 3, units = "in", dpi = 300)

```

Moving on to more validation analysis. More information could be extracted (e.g. which calls were most often grouped together, SPCC similarity of calls grouped together across observers), but I feel it doesn't add much more to the narrative of the manuscript at this point. Better for another manuscript on repeated individual sampling if that's of interest down the line.

## How did patterns of acoustic variation change across observers?

Perform a validation analysis in which we ask how patterns of acoustic variation are influenced at each of these 4 sites by putting in different numbers of potential repeated individuals, and number of calls per individual. 

Starting the analysis by asking how patterns of acoustic variation at each of these sites changes according to each observer, with SPCC similarity. Maybe report mean similarity per site, plus an MDS visual per site and observer? If so, then perform MDS on the full dataset, then subset by calls.
```{r echo = TRUE, eval = TRUE}

xc_mat_spec <- readRDS(file.path(path, "xc_mat_NAT_INT.RDS"))
# dim(xc_mat_spec) == nrow(nat_int_est)

# Make a vector of sound file names for substitution
wavs <- dimnames(xc_mat_spec)[[1]]
wavs <- gsub(".WAV-1", ".WAV", wavs)
# head(wavs)

# Substitute the SPCC matrix dimension names
dimnames(xc_mat_spec) <- list(wavs, wavs)

```

Per site-year and observer, randomly sample one call per potential repeated individual and retain all calls placed into "class_0" in order to get calls solely from "unique" individuals. Also get the calls per site that did not make it into the Shiny app.
```{r echo = TRUE, eval = FALSE}

# Since the data for the Shiny app was filtered to retain calls representing similarity values above the threshold, add back in the calls that were below this threshold per site

# Find calls that did go into the Shiny app
app_calls <- multi_obsv %>%
            dplyr::mutate(
              call = as.character(call),
              call = gsub("-1.jpeg", "", call)
            ) %>%
           pull(call) %>%
           unique()
length(app_calls)

# Per each of the multi-observer site-years, find the site scale calls that did not go into the Shiny app
non_app_calls <- nat_int_est %>%
  filter(social_scale == "Site") %>%
  filter(site_year %in% mo_sites) %>%
  filter(!sound.files %in% app_calls) %>%
  # The non-app calls must be repeated once per observer
  do(lapply(., rep, times = 
              length(multi_obsv %>%
                       pull(observer) %>%
                       unique())) %>% 
       as.data.frame()) %>%
  dplyr::mutate(
  observer = rep(
    multi_obsv %>% 
      pull(observer) %>%
      unique(), 
    each = nrow(.) / length(multi_obsv %>%
                                     pull(observer) %>%
                                     unique())
      ),
    class = "none"
  ) %>%
  dplyr::rename(
    call = sound.files
  ) %>%
  dplyr::select(names(multi_obsv))

glimpse(non_app_calls)

# Check that calls were repeated once per observer, looks good
# View(non_app_calls)

```

Make a data frame that contains mean acoustic similarity per site_year and observer after removing calls attributed to potential repeated individuals. Only consider potental repeated individuals that were assigned more than one call. Here, performed iterative random sampling of one call per potential repeated individual (sampling without replacement). Not bootstrapping, which relies on sampling with replacement.
```{r echo = TRUE, eval = FALSE}

# Iterate over site_years and observers
site_yrs <- unique(multi_obsv$site_year)
site_yrs

obsvs <- unique(multi_obsv$observer)
obsvs

# table(multi_obsv$observer)
# table(multi_obsv$site_year, multi_obsv$observer)

# Get randomly sampled site similarity (e.g. randomly sampling of one call without replacement per repeated individual per observer per site-year)
iter <- 1000

# i <- 4
# x <- 6
# z <- 1000
site_sim <- rbindlist(invisible(pblapply(1:length(site_yrs), function(i){
  
  iter_o_df <- rbindlist(lapply(1:length(obsvs), function(x){
    
    iter_i_df <- rbindlist(lapply(1:iter, function(z){
      
      # Potential repeated individuals with more than one call
      pri <- multi_obsv %>% 
        # Remove class_0, which was reserved for calls from "unique" individuals
        filter(class != "class_0") %>%
        # Only retain potental repeated individuals that were assigned more than one call
        group_by(site_year, observer, class) %>%
        dplyr::summarise(n_calls = length(call)) %>%
        ungroup() %>%
        filter(n_calls > 1) %>%
        # Filter by the given site_year
        filter(site_year == site_yrs[i]) %>%
        # Filter by the given observer
        filter(observer == obsvs[x]) %>%
        dplyr::select(-c(n_calls)) %>%
        # Join with the original data frame to recuperate sound files
        inner_join(
           multi_obsv,
           by = c("observer", "site_year", "class")
        )

      # Randomly sample one call per site_year, observer and potential repeated individual with more than one call
      # Random sampling is repeated over the number of iterations
      # set.seed(seed) # checking 
      pri_rsamp <- pri %>%
        group_by(site_year, observer, class) %>%
        nest() %>%
        ungroup() %>%
        dplyr::mutate(
        rsamp_call = purrr::map2(data, 1, sample_n, replace = FALSE)
        ) %>%
        dplyr::select(-data) %>%
        unnest(rsamp_call) %>%
        dplyr::select(observer, site_year, call, class)
      
      # Get all other calls for the given site-year: calls for any potential repeated individuals assigned only one call (if they exist), calls that were assigned to class_0, and calls that were lower than the SPCC threshold used to choose calls for visual classification

      # Get potential repeated individuals with only one call
      oneCall <- multi_obsv %>% 
        # Remove class_0, which was reserved for calls from "unique" individuals
        filter(class != "class_0") %>%
        # Only retain potental repeated individuals that were assigned more than one call
        group_by(site_year, observer, class) %>%
        dplyr::summarise(n_calls = length(call)) %>%
        ungroup() %>%
        filter(n_calls == 1) %>%
        # Filter by the given site_year
        filter(site_year == site_yrs[i]) %>%
        # Filter by the given observer
        filter(observer == obsvs[x]) %>%
        dplyr::select(-c(n_calls)) %>%
        # Join with the original data frame to recuperate sound files
        inner_join(
           multi_obsv,
           by = c("site_year", "observer", "class")
        )
        
      # Get class_0 calls
      c0_calls <-  multi_obsv %>% 
        filter(class == "class_0") %>%
        # Filter by the given site_year
        filter(site_year == site_yrs[i]) %>%
        # Filter by the given observer
        filter(observer == obsvs[x])
      
      # Get calls that did not go into the app
      no_app <- non_app_calls %>%
        # Filter by the given site_year
        filter(site_year == site_yrs[i]) %>%
        # Filter by the given observer
        filter(observer == obsvs[x])
      
      # Combine the randomly sampled calls per potential repeated individual with multiple calls with all others for the given site-year: calls for any potential repeated individuals assigned only one call (if they exist), calls that were assigned to class_0, and calls that were lower than the SPCC threshold used to choose calls for visual classification
      comb_calls <- pri_rsamp %>%
        bind_rows(oneCall) %>%
        bind_rows(c0_calls) %>%
        bind_rows(no_app)
       
      # Get calls for the current iteration  
      calls <- comb_calls %>%
        # Get rid of the image file extension in call names
        dplyr::mutate(
          call = as.character(call),
          call = gsub("-1.jpeg$", "", call)
        ) %>%
        # Get the calls themselves
        pull(call)
    
      # Subset the similarity matrix by these calls
      xc_mat_tmp <- xc_mat_spec[grep(paste(paste("^", calls, "$", sep = ""), collapse = "|"), dimnames(xc_mat_spec)[[1]]), grep(paste(paste("^", calls, "$", sep = ""), collapse = "|"), dimnames(xc_mat_spec)[[2]])]
      # dim(xc_mat_tmp) == length(calls)
    
      # Get the mean acoustic similarity within this site (represented by these calls)
      withn <- xc_mat_tmp[lower.tri(xc_mat_tmp, diag = FALSE)]
  
      tmp_df <- data.frame(site_year = site_yrs[i]) %>%
        dplyr::mutate(
          type = "Filtered",
          observer = obsvs[x],
          mean_acoustic_sim = mean(withn),
          se_acoustic_sim = std.error(withn),
          num_calls = length(calls),
          # Number of potential repeated individuals identified
          num_pri = nrow(pri_rsamp),
          iter = z
        )
    
      return(tmp_df)
      
    })) # Return results across iterations
  
    return(iter_i_df)
    
  })) # Return results across observers
  
  
  # Calculate mean similarity and 95 CIs per observer and site-year across iterations
  iter_summary <- iter_o_df %>%
    dplyr::select(-c(iter)) %>%
    group_by(type, observer, site_year, num_pri) %>%
    dplyr::summarise(mean_site_sim = mean(mean_acoustic_sim)) %>%
    ungroup() %>%
    inner_join(
      iter_o_df %>%
      dplyr::select(-c(iter)) %>%
      group_by(type, observer, site_year, num_pri) %>%
      dplyr::summarise(
        upper_CI = CI(mean_acoustic_sim, ci = 0.95)[1],
        lower_CI = CI(mean_acoustic_sim, ci = 0.95)[3]
        ) %>%
      ungroup(),
      by = c("type", "observer", "site_year", "num_pri")
    )
  
  # glimpse(iter_summary)
  
  # Get the similarity without filtering of potential repeated individuals (e.g. "baseline")
  # Search for all calls in the given site-year, calculate mean and standard error without filtering
  
  # Get calls for the given site year
  calls <- nat_int_est %>%
    filter(site_year == site_yrs[i]) %>%
    dplyr::mutate(
      call = as.character(sound.files),
      call = gsub("-1.jpeg", "", call)
    ) %>%
    pull(call)
  # calls
    
  # Subset the similarity matrix by these calls
  xc_mat_tmp <- xc_mat_spec[grep(paste(paste("^", calls, "$", sep = ""), collapse = "|"), dimnames(xc_mat_spec)[[1]]), grep(paste(paste("^", calls, "$", sep = ""), collapse = "|"), dimnames(xc_mat_spec)[[2]])]
  # dim(xc_mat_tmp) == length(calls)
    
  # Get the mean acoustic similarity within this site (represented by these calls)
  withn <- xc_mat_tmp[lower.tri(xc_mat_tmp, diag = FALSE)]
  
  # Join this info with data from Shiny app above
  comb_df <- data.frame(type = "Full") %>%
    dplyr::mutate(
      site_year = site_yrs[i],
      observer = "none",
      mean_site_sim = mean(withn),
      num_pri = 0,
      upper_CI = CI(withn, ci = 0.95)[1],
      lower_CI = CI(withn, ci = 0.95)[3]
    ) %>%
    bind_rows(iter_summary)
  
  # glimpse(comb_df)
    
  return(comb_df)
  
}))) # Return results across site-years

glimpse(site_sim)
# View(site_sim)

saveRDS(site_sim, file.path(path, "iter_site_sim_multiobsv.RDS"))

```

Make a visual across observers.
```{r echo = TRUE, eval = TRUE}

site_sim <- readRDS(file.path(path, "iter_site_sim_multiobsv.RDS"))
glimpse(site_sim)

# Change levels of site_year so native site comes first, and add range
# Also change levels of observer so "none" (e.g. not-filtered) comes last
site_sim <- site_sim %>%
  dplyr::mutate(
    site_year = as.character(site_year),
    range = ifelse(grepl("CISN", site_year), "Native", "Introduced"), 
    site_year = factor(site_year, levels = c("CISN_2017", "BALL_2004", "SOCC_2004", "SOCC_2019")),
    range = factor(range, levels = c("Native", "Introduced")),
    observer = factor(observer, levels = c("asm", "dlh", "eah", "gsv", "tfw", "vp", "none"))
  )
glimpse(site_sim)

# Make a dummy data frame to add a line for the non-filtered dataset (observer = "none") per facet
dum_df <- data.frame(
  site_year = factor(levels(site_sim$site_year)),
  Y = sapply(1:length(levels(site_sim$site_year)), function(i){
    site_sim %>%
      filter(observer == "none") %>%
      filter(grepl(levels(site_sim$site_year)[i], .$site_year)) %>%
      pull(mean_site_sim)
  })
)
glimpse(dum_df)

# Colors by range, hues by site_year for introduced range
fills <- scales::alpha(c("navy", "orange"), 0.85)

# The error bars are smaller than the symbol size
site_sim %>%
  ggplot(aes(x = observer, y = mean_site_sim)) +
  facet_grid(~ site_year, drop = TRUE, scales = "free_x", space = "free") +
  geom_hline(data = dum_df, aes(yintercept = Y), linetype = "dotted", color = "gray35") +
  geom_errorbar(aes(ymin = lower_CI, ymax = upper_CI), width = 0.2) +
  geom_point(aes(fill = range, color = range), shape = 21, size = 2) +
  scale_fill_manual(values = fills) +
  scale_color_manual(values = fills) +
  guides(fill = guide_legend(title = "Range", override.aes = list(size = 2)), color = guide_legend(title = "Range")) +
  xlab("Observer") +
  ylab("Mean SPCC similarity and 95% CI") +
  scale_y_continuous(limits = c(0, 0.75)) +
  theme_bw() + 
  theme(
    strip.text = element_text(size = 10),
    axis.title = element_text(size = 10),
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10, angle = 25),
    panel.grid.major = element_line(size = 0.15),
    panel.grid.minor = element_line(size = 0.15),
    axis.ticks = element_line(size = 0.15),
    legend.position = "top",
    legend.key.height = unit(0.35, "line"),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 10),
    legend.margin = margin(2, 2, 2, 2),
    legend.box.margin = margin(-5, -5, -5, -5)
  )

```

Make a visual across increasing numbers of potential repeated individuals (each of these is the number of classes identified per observer).
```{r echo = TRUE, eval = FALSE}

site_sim %>%
  ggplot(aes(x = num_pri, y = mean_site_sim)) +
  facet_grid(~ site_year, drop = TRUE) +
  geom_hline(data = dum_df, aes(yintercept = Y), linetype = "dotted", color = "gray35") +
  geom_errorbar(aes(ymin = lower_CI, ymax = upper_CI), width = 0.2) +
  geom_point(aes(fill = range, color = range), shape = 21, size = 2) +
  scale_fill_manual(values = fills) +
  scale_color_manual(values = fills) +
   guides(fill = guide_legend(title = "Range", override.aes = list(size = 2)), color = guide_legend(title = "Range")) +
  xlab("Number of Potential Repeated Individuals Identified") +
  ylab("Mean SPCC similarity and 95% CI") +
  scale_y_continuous(limits = c(0, 0.75)) +
  theme_bw() + 
  theme(
    strip.text = element_text(size = 10),
    axis.title = element_text(size = 10),
    axis.text = element_text(size = 10),
    panel.grid.major = element_line(size = 0.15),
    panel.grid.minor = element_line(size = 0.15),
    axis.ticks = element_line(size = 0.15),
    legend.position = "top",
    legend.key.height = unit(0.35, "line"),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 10),
    legend.margin = margin(2, 2, 2, 2),
    legend.box.margin = margin(-5, -5, -5, -5)
  )

```

Perform MDS on the full SPCC matrix, then subset by calls per site-year and obsever, make another graphic that has both the full and each observer data in a different panel, with the mean SPCC similarity printed in each strip label.
```{r echo = TRUE, eval = FALSE}

# Get the site scale calls for the sites used in the Shiny app only
calls <- nat_int_est %>%
  filter(social_scale == "Site") %>%
  filter(site_year %in% mo_sites) %>%
  pull(sound.files)
head(calls)
length(calls)

# Subset the SPCC matrix by these calls
xc_mat_tmp <- xc_mat_spec[grep(paste(paste("^", calls, "$", sep = ""), collapse = "|"), dimnames(xc_mat_spec)[[1]]), grep(paste(paste("^", calls, "$", sep = ""), collapse = "|"), dimnames(xc_mat_spec)[[2]])]
str(xc_mat_tmp)

# Convert to a distance matrix and dist object for isoMDS
dist_mat <- stats::as.dist(1 - xc_mat_tmp, diag = TRUE, upper = TRUE)
# str(dist_mat)

# Reduce dimensionality of the SPCC matrix to visualize calls in 2D acoustic space
iso <- invisible(MASS::isoMDS(dist_mat, k = 2, maxit = 1000, trace = FALSE))
# str(iso)
  
# Make a data frame with these low-dimensional space points from MDS
full_mds_df <- data.frame(X = iso$points[, 1], Y = iso$points[, 2]) %>%
  dplyr::mutate(
    type = "Full",
    observer = "none", 
    site_year = nat_int_est %>%
      filter(social_scale == "Site") %>%
      filter(site_year %in% mo_sites) %>%
      pull(site_year),
    class = "none",
    call = calls
    )
glimpse(full_mds_df)

# Randomly sample one call for plotting purposes
set.seed(seed)
filt_calls <- multi_obsv %>% 
  # Remove class_0, which was reserved for calls from "unique" individuals
  filter(class != "class_0") %>%
  # Only retain potental repeated individuals that were assigned more than one call
  group_by(site_year, observer, class) %>%
  dplyr::summarise(n_calls = length(call)) %>%
  ungroup() %>%
  filter(n_calls > 1) %>%
  dplyr::select(-c(n_calls)) %>%
  # Join with the original data frame to recuperate sound files
  inner_join(
      multi_obsv,
      by = c("site_year", "observer", "class")
  ) %>%
  group_by(site_year, observer, class) %>%
  nest() %>%
  ungroup() %>%
  dplyr::mutate(
    rsamp_call = purrr::map2(data, 1, sample_n, replace = FALSE)
  ) %>%
  dplyr::select(-data) %>%
  unnest(rsamp_call) %>%
  # Add back any calls from individuals assigned only a single call
  bind_rows(
    multi_obsv %>% 
    filter(class != "class_0") %>%
    group_by(site_year, observer, class) %>%
    dplyr::summarise(n_calls = length(call)) %>%
    ungroup() %>%
    filter(n_calls == 1) %>%
    dplyr::select(-c(n_calls)) %>%
    # Join with the original data frame to recuperate sound files
    inner_join(
      multi_obsv,
      by = c("site_year", "observer", "class")
    )
  ) %>%
  # Add back the class_0 calls per site_year, observer
  bind_rows(
    multi_obsv %>% 
    filter(class == "class_0")
  ) %>%
  # Add back the calls that did not make it into the app (below similarity threshold)
  bind_rows(non_app_calls) %>%
  # Get rid of the image file extension in call names
  dplyr::mutate(
    call = as.character(call),
    call = gsub("-1.jpeg$", "", call)
  )

glimpse(filt_calls)

# Combine the filtered calls per site-year with their respective MDS coordinates and join with the data frame of MDS coordinates of the full set of calls per site-year
comb_mds_df <- full_mds_df %>%
  bind_rows(
    filt_calls %>%
      dplyr::mutate(
        type = "Filtered"
      ) %>%
      left_join(
        full_mds_df %>%
          dplyr::select(call, X, Y), by = "call"
      )
  )

glimpse(comb_mds_df)

```

Make a visual of 2D acoustic space with kernel densities.
```{r echo = TRUE, eval = FALSE}

# Colors by range, hues by site_year for introduced range
fills <- alpha(c("navy", "sienna", "gold4", "goldenrod2"), 0.85)

comb_mds_df2 <- comb_mds_df %>%
  dplyr::mutate(
    site_year = as.character(site_year),
    range = ifelse(grepl("CISN", site_year), "Native", "Introduced"), 
    site_year = factor(site_year, levels = c("CISN_2017", "BALL_2004", "SOCC_2004", "SOCC_2019")),
    range = factor(range, levels = c("Native", "Introduced")),
    observer = factor(observer, levels = c("asm", "dlh", "eah", "gsv", "tfw", "vp", "none"))
  ) %>%
  dplyr::mutate(
    observer_site_year = paste(observer, site_year, sep = "_"),
    observer_site_year = factor(observer_site_year)
  ) %>%
  droplevels()
glimpse(comb_mds_df2)

# Faceted by site_year and observer
# Default for geom_density2d is to estimate optimal bandwidths using MASS::bandwidth.nrd()
ggA <- comb_mds_df2 %>%
  ggplot(aes(x = X, y = Y, fill = site_year, color = site_year)) +
  geom_density2d(n = 25, size = 0.50) +
  facet_grid(observer ~ site_year, switch = "y") +
  scale_fill_manual(values = fills) +
  scale_color_manual(values = fills) +
  xlab("Dimension 1") +
  ylab("Dimension 2") +
  guides(fill = FALSE, color = FALSE) +
  theme_bw() +
  theme(
    strip.text = element_text(size = 12),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 12),
    panel.grid.major = element_line(size = 0.15),
    panel.grid.minor = element_line(size = 0.15),
    axis.ticks = element_line(size = 0.15)
  )

ggA

# Faceted by observer only, introduced sites only
tmp_fills <- fills[-grep("CISN_2017", levels(comb_mds_df2$site_year))]

ggB <- comb_mds_df2 %>%
  filter(range == "Introduced") %>%
  droplevels() %>%
  ggplot(aes(x = X, y = Y, fill = site_year, color = site_year)) +
  geom_density2d(n = 25, size = 0.25) +
  facet_grid(rows = vars(observer), switch = "y") +
  scale_fill_manual(values = tmp_fills) +
  scale_color_manual(values = tmp_fills) +
  xlab("Dimension 1") +
  ylab("Dimension 2") +
  guides(fill = FALSE, color = FALSE) +
  theme_bw() +
  theme(
    strip.text = element_text(size = 12),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 12),
    panel.grid.major = element_line(size = 0.15),
    panel.grid.minor = element_line(size = 0.15),
    axis.ticks = element_line(size = 0.15)
  )

ggB

# Combine these two plots 
# ?ggarrange

gg_leg <- gtable::gtable_filter(ggplot_gtable(ggplot_build(
  comb_mds_df2 %>%
  ggplot(aes(x = X, y = Y)) +
  geom_line(aes(color = site_year), size = 3) +
  facet_grid(rows = vars(observer), switch = "y") +
  scale_color_manual(values = fills) +
  guides(color = guide_legend(title = "Site-Year")) +
  xlab("Dimension 1") +
  ylab("Dimension 2") +
  theme_bw() + 
  theme(
    legend.position = "top",
    legend.key.height = unit(0.35, "line"),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 12),
    legend.margin = margin(0, 0, 0, 0),
    legend.box.margin = margin(-10, -10, -10, -10),
    legend.key.width = unit(2, "lines")
  )
)), "guide-box")

# class(as.ggplot(gg_leg))
# grid::grid.draw(gg_leg)
dev.off()

# Arrange plots in a single image file 
ggarrange(
  as.ggplot(gg_leg),
  as.ggplot(
    ggarrange(
      ggA,
      ggB,
      nrow = 1,
      widths = c(8, 3)
    )
  ),
  nrow = 2,
  heights = c(1, 8)
)
```

The plots on the left show acoustic space per observer and site-year, the plots on the right show densities overlaid for the 3 introduced range site-years per observer.

# Comparing numbers of potential repeated individuals identified per range

Did we identify more potential repeated individuals for the introduced range, and more calls per each of these individuals? Get the potential repeated individuals across users.
```{r echo = TRUE, eval = TRUE}

file_nms <- list.files(pattern = ".json$", path)
file_nms

users <- sapply(1:length(file_nms), function(i){
  strsplit(file_nms[i], split = "-")[[1]][1]
})
users

# x <- 1
# i <- 12
# z <- 1
user_res_df <- rbindlist(pblapply(1:length(users), function(x){
  
  res <- list.load(file.path(path, file_nms[x]))
  # str(res)
  
  sy <- names(res[["Testing"]])

  # i <- 1
  res_df <- rbindlist(lapply(1:length(sy), function(i){
    
    # Bind list objects together into a wide data frame
    tmp_df <- do.call("cbind.data.frame", res[["Testing"]][[sy[i]]][[1]])
    # glimpse(tmp_df)
    
    # Convert to long format ot facilitate combining results across users
    tmp_df2 <- tmp_df %>%
      pivot_longer(cols = names(tmp_df)[grep("class_0|Potential_Individual", names(tmp_df))], names_to = "Class", values_to = "Call_image_files") %>%
      dplyr::mutate(
        Call_image_files = as.character(Call_image_files)
      )
    
    # glimpse(tmp_df2)

    # Iterate over potential repeated individuals and return all calls
    # z <- 1
    tmp_df3 <- rbindlist(lapply(1:nrow(tmp_df2), function(z){

      # Extract the individual filenames, remove the "-1.jpeg" suffix for grepping in EST later
      calls <- gsub("-1.jpeg$", "", strsplit(tmp_df2$Call_image_files[z], split = ";")[[1]])
      # calls
      
      # Return a data frame that has one row per call
      return(data.frame(login_username = tmp_df2$login_username[z], site_year = tmp_df2$page[z], Class = tmp_df2$Class[z], sound.files = calls))
      
    }))
  
    return(tmp_df3)
    
  }))  
  
  return(res_df)
  
}))

glimpse(user_res_df)
# user_res_df %>%
  # View()

```

# Repeated individual summary statistics per range

# SPCC threshold

How many total calls per range were >= the SPCC threshold (e.g. how many calls used in the app)?
```{r echo = TRUE, eval = TRUE}

# Get number of calls per range from Shiny prediction module calls
shiny_prediction_module_est <- readRDS(file.path(path, "shiny_potrepindiv_prediction_module_est.RDS"))
glimpse(shiny_prediction_module_est)

# More calls were in pairwise comparisons greater than or equal the SPCC threshold for the introduced range
# Note that there's one unique sound file per row, multiobserver sites were encoded with all observer names in the co-author column
unique(shiny_prediction_module_est$co_author_initials)

shiny_prediction_module_est %>%
  group_by(range) %>%
  dplyr::summarise(
    n_calls = length(sound.files)
  )

# # A tibble: 2 x 2
#   range    n_calls
#   <fct>      <int>
# 1 Native       478
# 2 Introduced     635

```

## Gaussian mixture modelling

How many potential repeated individuals were identified on average per range by Gaussian mixture modeling? Here, find unique potential repeated individuals (clusters) that have more than one call, then get mean and standard error of the number of repeated individuals identified across site-years and range per range.
```{r echo = TRUE, eval = TRUE}

shiny_prediction_module_est %>%
  # Get the number of calls assigned to each cluster identified per range and site-year
  group_by(range, site_year, cluster) %>%
  dplyr::summarise(
    n_calls = length(sound.files)
  ) %>%
  # Retain clusters that were assigned more than 1 call
  filter(n_calls > 1) %>%
  # Rejoin to the full data frame to regain sound files
  inner_join(
    shiny_prediction_module_est,
    by = c("range", "site_year", "cluster")
  ) %>%
  # How many clusters were identified per site-year?
  group_by(range, site_year) %>%
  dplyr::summarise(
    n_clust = n_distinct(cluster)
  ) %>%
  # Mean and standard error per range?
  group_by(range) %>%
  dplyr::summarise(
    mean_clusters = round(mean(n_clust), 2),
    se_clusters = round(std_err(n_clust), 2)
    # median_clusters = round(median(n_clust), 2),
    # IQR_clusters = round(IQR(n_clust), 2)
  )

# Mean and SE
# # A tibble: 2 x 3
#   range    mean_clusters se_clusters
#   <fct>            <dbl>       <dbl>
# 1 Native            3.24        0.38
# 2 Introduced          3.4         0.47

# Median and IQR
# # A tibble: 2 x 3
#   range    median_clusters IQR_clusters
#   <fct>              <dbl>        <dbl>
# 1 Native                 2            4
# 2 Introduced               4            4

# Maximum cluster identified per range, 8 in each
shiny_prediction_module_est %>%
  # Get the number of calls assigned to each cluster identified per range and site-year
  group_by(range, site_year, cluster) %>%
  dplyr::summarise(
    n_calls = length(sound.files)
  ) %>%
  # Retain clusters that were assigned more than 1 call
  filter(n_calls > 1) %>%
  # Rejoin to the full data frame to regain sound files
  inner_join(
    shiny_prediction_module_est,
    by = c("range", "site_year", "cluster")
  ) %>%
  # How many clusters were identified per site-year?
  group_by(range, site_year) %>%
  dplyr::summarise(
    n_clust = n_distinct(cluster)
  ) %>%
  group_by(range) %>%
  dplyr::summarise(
    max_clusters = max(n_clust)
  )

# But larger numbers of individuals were less frequent
shiny_prediction_module_est %>%
  # Get the number of calls assigned to each cluster identified per range and site-year
  group_by(range, site_year, cluster) %>%
  dplyr::summarise(
    n_calls = length(sound.files)
  ) %>%
  # Retain clusters that were assigned more than 1 call
  filter(n_calls > 1) %>%
  # Rejoin to the full data frame to regain sound files
  inner_join(
    shiny_prediction_module_est,
    by = c("range", "site_year", "cluster")
  ) %>%
  # How many clusters were identified per site-year?
  group_by(range, site_year) %>%
  dplyr::summarise(
    n_clust = n_distinct(cluster)
  ) %>%
  pull(n_clust) %>%
  hist()

```

How calls were attributed to potential repeated individuals were identified on average per range by Gaussian mixture modeling? Here, find unique potential repeated individuals (clusters) that have more than one call, then get mean and standard error of the number calls attributed to repeated individuals identified across site-years and range per range.
```{r echo = TRUE, eval = TRUE}

shiny_prediction_module_est %>%
  # Get the number of calls assigned to each cluster identified per range and site-year
  group_by(range, site_year, cluster) %>%
  dplyr::summarise(
    n_calls = length(sound.files)
  ) %>%
  # Retain clusters that were assigned more than 1 call
  filter(n_calls > 1) %>%
  # Rejoin to the full data frame to regain sound files
  inner_join(
    shiny_prediction_module_est,
    by = c("range", "site_year", "cluster")
  ) %>%
  # How many clusters were identified per site-year?
  group_by(range, site_year) %>%
  dplyr::summarise(
    n_calls = length(sound.files)
  ) %>%
  # Mean and standard error per range?
  group_by(range) %>%
  dplyr::summarise(
    mean_calls = round(mean(n_calls), 2),
    se_calls = round(std_err(n_calls), 2)
    # median_calls = round(median(n_calls), 2),
    # IQR_calls = round(IQR(n_calls), 2)
  )

# Mean and SE
# # A tibble: 2 x 3
#   range    mean_calls se_calls
#   <fct>         <dbl>    <dbl>
# 1 Native         10.4     1.61
# 2 Introduced       23.6     5.53

# Median and IQR
# # A tibble: 2 x 3
#   range    median_calls IQR_calls
#   <fct>           <dbl>     <dbl>
# 1 Native              5        15
# 2 Introduced           14        43


```

## Visual classification

How many total potential repeated individuals were identified on average per range during visual classification? Here, find unique potential repeated individuals that have more than one call.

Total number of potential repated individuals identified per range.
```{r echo = TRUE, eval = FALSE}

# Join back with the full EST to get the range column
# Drop the multiobserver sites for now
# Get unique potential repeated individuals that have more than one call
user_res_df %>%
  filter(!site_year %in% c("CISN_2017", "BALL_2004", "SOCC_2004", "SOCC_2019")) %>%
  # Drop class_0 calls
  filter(Class != "class_0") %>%
  inner_join(
    nat_int_est %>%
      as_tibble() %>%
      dplyr::select(c(range, sound.files)),
    by = "sound.files"
  ) %>% 
  dplyr::mutate(
    uniq_id = paste(range, site_year, Class, sep = "_")
  ) %>%
  group_by(range, uniq_id) %>%
  dplyr::summarise(
    n_calls = length(sound.files)
  ) %>%
  ungroup() %>%
  filter(n_calls > 1) %>%
  group_by(range) %>%
  dplyr::summarise(
    n_pot_rep_indivs = length(uniq_id)
  )

# # A tibble: 2 x 2
#   range    n_pot_rep_indivs
#   <fct>               <int>
# 1 Native                101
# 2 Introduced               75

```

Mean and standard error or median and IQR of the number of potential repeatedly sampled individuals across site-years per range.
```{r echo = TRUE, eval = FALSE}

user_res_df %>%
  filter(!site_year %in% c("CISN_2017", "BALL_2004", "SOCC_2004", "SOCC_2019")) %>%
  # Drop class_0 calls
  filter(Class != "class_0") %>%
  inner_join(
    nat_int_est %>%
      as_tibble() %>%
      dplyr::select(c(range, sound.files)),
    by = "sound.files"
  ) %>% 
  dplyr::mutate(
    uniq_id = paste(range, site_year, Class, sep = "_")
  ) %>%
  group_by(range, site_year, uniq_id) %>%
  dplyr::summarise(
    n_calls = length(sound.files)
  ) %>%
  ungroup() %>%
  filter(n_calls > 1) %>%
  group_by(range, site_year) %>%
  dplyr::summarise(
    n_pot_rep_indivs = length(uniq_id)
  ) %>%
  group_by(range) %>%
  dplyr::summarise(
    mean_npri = round(mean(n_pot_rep_indivs), 2),
    se_npri = round(std_err(n_pot_rep_indivs), 2)
    # median_npri = round(median(n_pot_rep_indivs), 2),
    # IQR_npri = round(IQR(n_pot_rep_indivs), 2)
  )

# Mean and SE
# # A tibble: 2 x 3
#   range    mean_npri se_npri
#   <fct>        <dbl>   <dbl>
# 1 Native        3.48    0.39
# 2 Introduced      3.57    0.54

# Median and IQR, less informative than mean and SE
# # A tibble: 2 x 3
#   range    median_npri IQR_npri
#   <fct>          <dbl>    <dbl>
# 1 Native             3        3
# 2 Introduced           3        3

```

Mean and standard error of the number of calls per potential repeatedly sampled individuals across site-years per range.
```{r echo = TRUE, eval = FALSE}

user_res_df %>%
  filter(!site_year %in% c("CISN_2017", "BALL_2004", "SOCC_2004", "SOCC_2019")) %>%
  # Drop class_0 calls
  filter(Class != "class_0") %>%
  inner_join(
    nat_int_est %>%
      dplyr::select(c(range, sound.files)),
    by = "sound.files"
  ) %>% 
  dplyr::mutate(
    uniq_id = paste(range, site_year, Class, sep = "_")
  ) %>%
  group_by(range, site_year, uniq_id) %>%
  dplyr::summarise(
    n_calls = length(sound.files)
  ) %>%
  ungroup() %>%
  filter(n_calls > 1) %>%
  group_by(range) %>%
  dplyr::summarise(
    mean_calls = round(mean(n_calls), 2),
    se_calls = round(std_err(n_calls), 2)
  )

# # A tibble: 2 x 3
#   range    mean_calls se_calls
#   <fct>         <dbl>    <dbl>
# 1 Native         2.83    0.15
# 2 Introduced       5.31    0.64

```

More calls were attributed to potential repeated indivduals in the introduced range. But could this be due to some observers handling more introduced range site-years than others? The number of site-years assessed per observer should have been pretty similar given our random sampling.
```{r echo = TRUE, eval = FALSE}

# Similar numbers of site-years were assessed per range across observers
user_res_df %>%
  filter(!site_year %in% c("CISN_2017", "BALL_2004", "SOCC_2004", "SOCC_2019")) %>%
  inner_join(
    nat_int_est %>%
      dplyr::select(c(range, sound.files)),
    by = "sound.files"
  ) %>% 
  group_by(range, login_username) %>%
  dplyr::summarise(
    num_sy = n_distinct(site_year)
  ) %>%
  pivot_wider(
    names_from = "range",
    values_from = "num_sy"
  )

```

Mean and standard error of site-years assessed across observers per range.
```{r echo = TRUE, eval = FALSE}

user_res_df %>%
  filter(!site_year %in% c("CISN_2017", "BALL_2004", "SOCC_2004", "SOCC_2019")) %>%
  inner_join(
    nat_int_est %>%
      dplyr::select(c(range, sound.files)),
    by = "sound.files"
  ) %>% 
  group_by(range, login_username) %>%
  dplyr::summarise(
    num_sy = n_distinct(site_year)
  ) %>%
  dplyr::summarise(
    mean_sy = round(mean(num_sy), 2),
    se_sy = round(std_err(num_sy), 2)
  )

# # A tibble: 2 x 3
#   range    mean_sy se_sy
#   <fct>      <dbl> <dbl>
# 1 Native      6     0.26
# 2 Introduced    3.67  0.33

```

Mean and standard error of the number of potential repeated individuals assessed per observer per range.
```{r echo = TRUE, eval = FALSE}

user_res_df %>%
  filter(!site_year %in% c("CISN_2017", "BALL_2004", "SOCC_2004", "SOCC_2019")) %>%
  # Drop class_0 calls
  filter(Class != "class_0") %>%
  inner_join(
    nat_int_est %>%
      dplyr::select(c(range, sound.files)),
    by = "sound.files"
  ) %>% 
  dplyr::mutate(
    uniq_id = paste(range, site_year, Class, sep = "_")
  ) %>%
  group_by(range, login_username, site_year, uniq_id) %>%
  dplyr::summarise(
    n_calls = length(sound.files)
  ) %>%
  ungroup() %>%
  filter(n_calls > 1) %>%
  group_by(range, login_username, site_year) %>%
  dplyr::summarise(
    n_pot_rep_indivs = length(uniq_id)
  ) %>%
  group_by(range, login_username) %>%
  dplyr::summarise(
    mean_i = round(mean(n_pot_rep_indivs), 2),
    se_i = round(std_err(n_pot_rep_indivs), 2)
  ) %>%
   pivot_wider(
    names_from = "range",
    values_from = c("mean_i", "se_i")
  ) %>%
  dplyr::rename(
    observer = login_username
  ) %>%
  dplyr::mutate(
    Native = paste(mean_i_Native, "+/-", se_i_Native, sep = " "),
    Introduced = paste(mean_i_Introduced, "+/-", se_i_Introduced, sep = " ")
  ) %>%
  dplyr::select(
    c(observer, Native, Introduced)
  )

```

Mean and standard error of the number of calls attributed to potential repeated individuals per observer per range.
```{r echo = TRUE, eval = FALSE}

# Mean and SE calls per pot rep indiv per observer and range
user_res_df %>%
  filter(!site_year %in% c("CISN_2017", "BALL_2004", "SOCC_2004", "SOCC_2019")) %>%
  # Drop class_0 calls
  filter(Class != "class_0") %>%
  inner_join(
    nat_int_est %>%
      dplyr::select(c(range, sound.files)),
    by = "sound.files"
  ) %>% 
  dplyr::mutate(
    uniq_id = paste(range, site_year, Class, sep = "_")
  ) %>%
  group_by(range, login_username, site_year, uniq_id) %>%
  dplyr::summarise(
    n_calls = length(sound.files)
  ) %>%
  ungroup() %>%
  filter(n_calls > 1) %>%
  group_by(range, login_username) %>%
  dplyr::summarise(
    mean_c = round(mean(n_calls), 2),
    se_c = round(std_err(n_calls), 2)
  ) %>%
  pivot_wider(
    names_from = "range",
    values_from = c("mean_c", "se_c")
  ) %>%
  dplyr::rename(
    observer = login_username
  ) %>%
  dplyr::mutate(
    Native = paste(mean_c_Native, "+/-", se_c_Native, sep = " "),
    Introduced = paste(mean_c_Introduced, "+/-", se_c_Introduced, sep = " ")
  ) %>%
  dplyr::select(
    c(observer, Native, Introduced)
  )

```

Overall, there were more mean numbers of potential repeated individuals identified per site in the introduced range, although this wasn't a very large difference. However, more calls were attributed of potential repeated individuals on average, and the difference between ranges was quite pronounced. 

# References

    1. Smith-Vidaurre, G., Araya-Salas, M., and T.F. Wright. 2020. Individual signatures outweigh social group identity in contact calls of a communally nesting parrot. Behavioral Ecology 31(2), 448-458. https://doi.org/10.1093/beheco/arz202
    
    2. Smith-Vidaurre, G., Perez-Marrufo, V., & Wright, T. F. 2021. Individual vocal signatures show reduced complexity following invasion. Animal Behavior, 179, 15–39. https://doi.org/10.1016/j.anbehav.2021.06.020

```{r echo = TRUE, eval = TRUE}

sessionInfo()

```
